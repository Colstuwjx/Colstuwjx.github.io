<!doctype html><html lang=zh-cn>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=generator content="Hugo 0.87.0 with theme Tranquilpeak 0.4.7-BETA">
<meta name=author content="Jacky Wu (Colstuwjx)">
<meta name=keywords content>
<meta name=description content="之前有写过一篇试玩 containerd 的文章，一晃已经3年多了，业内已经有不少人选择抛弃 docker，直接换成 containerd 了。

那么，这玩意儿到底好使不，切换过程中又会有哪些问题呢？今天笔者就来分享一下最近从 docker 迁移到 containerd 做的一些工作，以及个人在这个过程中的一些思考。">
<meta property="og:description" content="之前有写过一篇试玩 containerd 的文章，一晃已经3年多了，业内已经有不少人选择抛弃 docker，直接换成 containerd 了。

那么，这玩意儿到底好使不，切换过程中又会有哪些问题呢？今天笔者就来分享一下最近从 docker 迁移到 containerd 做的一些工作，以及个人在这个过程中的一些思考。">
<meta property="og:type" content="article">
<meta property="og:title" content="[ 原创 ] containerd 迁移二三事">
<meta name=twitter:title content="[ 原创 ] containerd 迁移二三事">
<meta property="og:url" content="https://colstuwjx.github.io/2021/08/%E5%8E%9F%E5%88%9B-containerd-%E8%BF%81%E7%A7%BB%E4%BA%8C%E4%B8%89%E4%BA%8B/">
<meta property="twitter:url" content="https://colstuwjx.github.io/2021/08/%E5%8E%9F%E5%88%9B-containerd-%E8%BF%81%E7%A7%BB%E4%BA%8C%E4%B8%89%E4%BA%8B/">
<meta property="og:site_name" content="Colstuwjx's site">
<meta property="og:description" content="之前有写过一篇试玩 containerd 的文章，一晃已经3年多了，业内已经有不少人选择抛弃 docker，直接换成 containerd 了。

那么，这玩意儿到底好使不，切换过程中又会有哪些问题呢？今天笔者就来分享一下最近从 docker 迁移到 containerd 做的一些工作，以及个人在这个过程中的一些思考。">
<meta name=twitter:description content="之前有写过一篇试玩 containerd 的文章，一晃已经3年多了，业内已经有不少人选择抛弃 docker，直接换成 containerd 了。

那么，这玩意儿到底好使不，切换过程中又会有哪些问题呢？今天笔者就来分享一下最近从 docker 迁移到 containerd 做的一些工作，以及个人在这个过程中的一些思考。">
<meta property="og:locale" content="zh-cn">
<meta property="article:published_time" content="2021-08-23T08:00:00">
<meta property="article:modified_time" content="2021-08-23T08:00:00">
<meta property="article:section" content="cloud-native">
<meta property="article:tag" content="cloud-native">
<meta property="article:tag" content="containerd">
<meta property="article:tag" content="docker">
<meta name=twitter:card content="summary">
<meta property="og:image" content="https://res.cloudinary.com/ddhitj0ja/image/upload/v1581899652/me_jfkg8t.jpg">
<meta property="twitter:image" content="https://res.cloudinary.com/ddhitj0ja/image/upload/v1581899652/me_jfkg8t.jpg">
<title>[ 原创 ] containerd 迁移二三事</title>
<link rel=icon href=https://colstuwjx.github.io/favicon.png>
<link rel=canonical href=https://colstuwjx.github.io/2021/08/%E5%8E%9F%E5%88%9B-containerd-%E8%BF%81%E7%A7%BB%E4%BA%8C%E4%B8%89%E4%BA%8B/>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous>
<link rel=stylesheet href=https://colstuwjx.github.io/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css>
<link rel=stylesheet href=https://colstuwjx.github.io/css/comments.css>
</head>
<body>
<div id=blog>
<header id=header data-behavior=4>
<i id=btn-open-sidebar class="fa fa-lg fa-bars"></i>
<div class=header-title>
<a class=header-title-link href=https://colstuwjx.github.io/>Colstuwjx's site</a>
</div>
<a class=header-right-picture href=https://colstuwjx.github.io/#about>
<img class=header-picture src=https://res.cloudinary.com/ddhitj0ja/image/upload/v1581899652/me_jfkg8t.jpg alt=作者的图片>
</a>
</header>
<nav id=sidebar data-behavior=4>
<div class=sidebar-container>
<div class=sidebar-profile>
<a href=https://colstuwjx.github.io/#about>
<img class=sidebar-profile-picture src=https://res.cloudinary.com/ddhitj0ja/image/upload/v1581899652/me_jfkg8t.jpg alt=作者的图片>
</a>
<h4 class=sidebar-profile-name>Jacky Wu (Colstuwjx)</h4>
<h5 class=sidebar-profile-bio>DevOPS & OpenSource Fans</h5>
</div>
<ul class=sidebar-buttons>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://colstuwjx.github.io/>
<i class="sidebar-button-icon fa fa-lg fa-home"></i>
<span class=sidebar-button-desc>首页</span>
</a>
</li>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://colstuwjx.github.io/categories>
<i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
<span class=sidebar-button-desc>分类</span>
</a>
</li>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://colstuwjx.github.io/tags>
<i class="sidebar-button-icon fa fa-lg fa-tags"></i>
<span class=sidebar-button-desc>标签</span>
</a>
</li>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://colstuwjx.github.io/archives>
<i class="sidebar-button-icon fa fa-lg fa-archive"></i>
<span class=sidebar-button-desc>归档</span>
</a>
</li>
</ul>
<ul class=sidebar-buttons>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://github.com/Colstuwjx target=_blank rel=noopener>
<i class="sidebar-button-icon fa fa-lg fa-github"></i>
<span class=sidebar-button-desc>GitHub</span>
</a>
</li>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://colstuwjx.github.io/#about>
<i class="sidebar-button-icon fa fa-lg fa-user"></i>
<span class=sidebar-button-desc>关于</span>
</a>
</li>
<li class=sidebar-button>
<a class=sidebar-button-link href=https://colstuwjx.github.io/index.xml>
<i class="sidebar-button-icon fa fa-lg fa-rss"></i>
<span class=sidebar-button-desc>RSS</span>
</a>
</li>
</ul>
<ul class=sidebar-buttons>
</ul>
</div>
</nav>
<div id=main data-behavior=4 class=hasCoverMetaIn>
<article class=post itemscope itemtype=http://schema.org/BlogPosting>
<div class="post-header main-content-wrap text-left">
<h1 class=post-title itemprop=headline>
[ 原创 ] containerd 迁移二三事
</h1>
</div>
<div class="post-content markdown" itemprop=articleBody>
<div class=main-content-wrap>
<p>之前有写过一篇<a href=https://colstuwjx.github.io/tasting-containerd-01/>试玩 containerd </a>的文章，一晃已经3年多了，业内已经有不少人选择抛弃 docker，直接换成 containerd 了。</p>
<p>那么，这玩意儿到底好使不，切换过程中又会有哪些问题呢？今天笔者就来分享一下最近从 docker 迁移到 containerd 做的一些工作，以及个人在这个过程中的一些思考。</p>
<h2 id=背景>背景</h2>
<p>首先介绍一下这件事情的背景。我们内部有运行多套 k8s 集群，为了消费 v1.20 版本引入的一些新特性，我们打算升级一下集群的版本。然而，一旦升级到 v1.20 ，官方已经明确表示将会逐渐弃用对 <code>dockershim</code> 的支持。要不要把运行时也一起切掉呢？一番讨论过后，我们最终还是决定趁着这次升级集群的机会，把运行时也切换到 <code>containerd</code> 。</p>
<p><em>注：关于弃用 dockershim 这块，笔者也专门写了一篇<a href=https://colstuwjx.github.io/dive-into-sourcecode-why-k8s-deprecated-dockershim/>文章</a>分析代码层面维护 dockershim 的成本，有兴趣的朋友可以看看</em></p>
<h2 id=几个问题>几个问题</h2>
<h3 id=变更配置-启用-cri-plugin>变更配置：启用 cri plugin</h3>
<p>我们内部是用 <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a> 这套 Ansible 脚本来管理 k8s 集群的。从 docker 切换到 containerd 似乎只是改一行 playbook 变量的问题。</p>
<p>然而，实际当然没有这么简单。首先遇到的第一个问题便是配置的问题：docker 此前的部署默认会禁用 containerd 有关 CRI 的 plugin 。切换到直接使用 containerd 以后，我们需要手动修改 containerd 的相关配置：</p>
<pre tabindex=0><code>$ vim /etc/containerd/config.toml
#disabled_plugins = [&#34;cri&#34;]</code></pre>
<p>此外，我们在测试环境把 docker 切换到 containerd 以后发现之前的 docker 容器还有一些残留，这也需要运维手动清理。</p>
<p><em>注：如何将运行时自动安全地从 docker 切换到 containerd，可以参考 eBay 分享的这篇文章：<a href=https://www.infoq.cn/article/odslclsjvo8bnx*mbrbk>https://www.infoq.cn/article/odslclsjvo8bnx*mbrbk</a></em></p>
<h3 id=监控适配-cadvisor>监控适配：cadvisor</h3>
<p>整个切换的过程除了中间的一点小插曲倒还算顺利，跑了几天发现似乎也挺稳定的（毕竟 docker 也是用的这个 runtime ）。这是不是就完事了呢？等一下，咦，监控好像没数据了！什么情况？</p>
<p>原来，我们当初落地 k8s 的时候还搞了几个配套的周边组件，用来解决监控日志之类的需求，其中就有用到 cadvisor 来采集容器层面的基础指标，比如 CPU、内存使用率等等。</p>
<p>为了区分每个业务应用的数据，便于做单独的监控和告警，笔者在 cadvisor 这一层也做了一些改动，详细信息见 <a href=https://github.com/google/cadvisor/pull/2330>PR #2330</a>，主要就是设置了一个容器 env 白名单，然后把这些 env 变量作为各项监控指标的 label 对外提供，这样我们便可以传入一些标识应用的环境变量，比如 <code>APP_ID</code>，方便做应用维度的监控告警。</p>
<p>但是，从这个 PR 的<a href=https://github.com/google/cadvisor/pull/2330#issuecomment-545656563>评论部分</a>也可以看到，社区开发人员指出，把容器的 env 作为标签加到 cadvisor 对外暴露的 <code>/metrics</code> 接口的话，不只是 docker，containerd 和其他运行时也都会受影响。因此，他建议把 <code>docker_env_metadata_whitelist</code> 这个参数改成统一的 <code>env_metadata_whitelist</code>，这样所有容器运行时都可以消费这个 flag。</p>
<p>当时因为时间精力有限，笔者选择的是暂时先搁置这个 PR，手动在自己 fork 的 repo 里 merge 了<a href=https://github.com/Colstuwjx/cadvisor/commit/96c96bb801a85bdf3b2f88a605a9053a1f95806d>该 commit </a>，随后在我们内网环境使用这份修改后的版本。然而时隔将近两年后，因为切换到 containerd 的缘故，终究还是得把这件事情做完。</p>
<p>与此同时，笔者又想到另外一个问题，cadvisor 对于 containerd 的支持力度到底怎么样？</p>
<blockquote>
<p>第一个问题就是容器启停这块的事件处理机制，cadvisor 一般是作为一个 DaemonSet 运行在宿主机上，这就不可避免地遇到一个问题：怎么知道自己监控的对象？</p>
</blockquote>
<p>查看代码后不难发现，cadvisor 是有实现一套统一的 <a href=https://github.com/google/cadvisor/blob/v0.40.0/watcher/watcher.go#L45>ContainerWatcher</a> 的接口：</p>
<pre tabindex=0><code>type ContainerWatcher interface {
	// Registers a channel to listen for events affecting subcontainers (recursively).
	Start(events chan ContainerEvent) error

	// Stops watching for subcontainer changes.
	Stop() error
}</code></pre>
<p>它似乎约定了每个容器运行时得自己去实现各自的 watch 机制，那么，是否真的是这样呢？我们不妨接着看下去：</p>
<pre tabindex=0><code>...
// Listen to events from the container handler.
go func() {
    for {
        select {
        case event := &lt;-m.eventsChannel:
            switch {
            case event.EventType == watcher.ContainerAdd:
                switch event.WatchSource {
                default:
                    // 监听新加容器的事件并响应
                    err = m.createContainer(event.Name, event.WatchSource)
                }
            case event.EventType == watcher.ContainerDelete:
                // 监听删除容器的事件并响应
                err = m.destroyContainer(event.Name)
            }
            if err != nil {
                klog.Warningf(&#34;Failed to process watch event %+v: %v&#34;, event, err)
            }
        case &lt;-quit:
            // 停止之前启动的各个 watcher，然后平滑退出
            ...
        }
    }
}()
...</code></pre>
<p>如上，笔者在 cadvisor 业务逻辑的入口实现 <a href=https://github.com/google/cadvisor/blob/v0.40.0/manager/manager.go>manager.go</a> 里面找到了相关事件的<a href=https://github.com/google/cadvisor/blob/v0.40.0/manager/manager.go#L1168>处理循环</a>，见上面笔者添加的一些注释，似乎就是中规中矩的监听各个添加或删除的事件然后执行相应的操作。那么，我们不妨看看各个容器运行时具体的 handler 是怎么实现的吧！</p>
<pre tabindex=0><code>...
container.RegisterContainerHandlerFactory(f, []watcher.ContainerWatchSource{watcher.Raw})
...</code></pre>
<p>咦？无论是 <a href=https://github.com/google/cadvisor/blob/v0.40.0/container/docker/factory.go#L388>docker</a>，还是 <a href=https://github.com/google/cadvisor/blob/v0.40.0/container/docker/factory.go#L388>containerd</a>，又或者是 <a href=https://github.com/google/cadvisor/blob/v0.40.0/container/crio/factory.go#L160>cri-o</a>，全都是注册的 Raw 这个 watcher，根本没有去实现各自的 <code>ContainerWatcher</code>。</p>
<p>且继续看下去。笔者再次找到 manager 这块的代码，可以看到，各个容器运行时貌似是有机会去注册单独的 <code>watcher</code>：</p>
<pre tabindex=0><code>// Start the container manager.
func (m *manager) Start() error {
    // 各个容器运行时自己提供的 watcher 的初始化
	m.containerWatchers = container.InitializePlugins(m, m.fsInfo, m.includedMetrics)

    ...

    rawWatcher, err := raw.NewRawContainerWatcher()
    if err != nil {
        return err
    }
    // 单独追加的 raw watcher
    m.containerWatchers = append(m.containerWatchers, rawWatcher)

    ...
}</code></pre>
<p>但是，翻到具体运行时代码的时候就会发现，它压根就没实现，下面即是 docker 注册自己的时候实现的 <code>Register</code> 方法：</p>
<pre tabindex=0><code>func (p *plugin) Register(factory info.MachineInfoFactory, fsInfo fs.FsInfo, includedMetrics container.MetricSet) (watcher.ContainerWatcher, error) {
	err := Register(factory, fsInfo, includedMetrics)
	return nil, err
}</code></pre>
<p>哈哈，直接返回的 <code>nil</code>！那它怎么拿到容器的生命周期，然后做监控采集的处理呢？答案就在于 manager 单独加进去的这个 raw watcher。</p>
<p>我们不妨一起来看看这个 raw watcher 具体的代码实现。watcher 最开始会初始化一个 inotify 的 watcher，然后会给出对应的 cgroup 挂载路径：</p>
<pre tabindex=0><code>func NewRawContainerWatcher() (watcher.ContainerWatcher, error) {
    ...
	watcher, err := common.NewInotifyWatcher()
	if err != nil {
		return nil, err
	}

	rawWatcher := &amp;rawContainerWatcher{
		cgroupPaths:      common.MakeCgroupPaths(cgroupSubsystems.MountPoints, &#34;/&#34;),
		cgroupSubsystems: &amp;cgroupSubsystems,
		watcher:          watcher,
		stopWatcher:      make(chan error),
	}

	return rawWatcher, nil
}</code></pre>
<p>在 watcher 启动时，watcher 会去遍历指定的 cgroup 目录，然后注册对应的监听器，事件源即是之前初始化的 inotify watcher 监听到目录变化后发出的事件：</p>
<pre tabindex=0><code>func (w *rawContainerWatcher) Start(events chan watcher.ContainerEvent) error {
	// Watch this container (all its cgroups) and all subdirectories.
	watched := make([]string, 0)
    // watch 给定的 cgroup 路径，然后维护一组监听器
	for _, cgroupPath := range w.cgroupPaths {
		_, err := w.watchDirectory(events, cgroupPath, &#34;/&#34;)
		if err != nil {
			for _, watchedCgroupPath := range watched {
				_, removeErr := w.watcher.RemoveWatch(&#34;/&#34;, watchedCgroupPath)
				if removeErr != nil {
					klog.Warningf(&#34;Failed to remove inotify watch for %q with error: %v&#34;, watchedCgroupPath, removeErr)
				}
			}
			return err
		}
		watched = append(watched, cgroupPath)
	}

	// Process the events received from the kernel.
	go func() {
		for {
			select {
			case event := &lt;-w.watcher.Event():
                // 处理获取到的事件
				err := w.processEvent(event, events)
				if err != nil {
					klog.Warningf(&#34;Error while processing event (%+v): %v&#34;, event, err)
				}
            ...
            }
        }
    }()
    ...
}</code></pre>
<p>为什么会用 cgroup + inotify 这套机制呢？其实仔细想想也挺合理，cadvisor 开发的时候 CRI 标准都还没有呢，要是指望运行时这层给出一套统一的事件接口再来开发，那黄花菜都凉了。</p>
<blockquote>
<p>除此之外，还有一个问题就是，cadvisor 怎么采集 containerd 的 metrics 呢？</p>
</blockquote>
<p>翻阅代码之后一目了然：</p>
<pre tabindex=0><code>func (h *containerdContainerHandler) GetStats() (*info.ContainerStats, error) {
    // 可以看到 contaienrd 是直接复用 libcontainer 的逻辑来采集监控数据
    // 当然，docker 也是通过这种方式做的，具体可以看下 https://github.com/google/cadvisor/blob/v0.40.0/container/docker/handler.go#L460
	stats, err := h.libcontainerHandler.GetStats()
	if err != nil {
		return stats, err
	}
	// Clean up stats for containers that don&#39;t have their own network - this
	// includes containers running in Kubernetes pods that use the network of the
	// infrastructure container. This stops metrics being reported multiple times
	// for each container in a pod.
	if !h.needNet() {
		stats.Network = info.NetworkStats{}
	}

	// Get filesystem stats.
	err = h.getFsStats(stats)
	return stats, err
}</code></pre>
<p>正如注释说的，它<a href=https://github.com/google/cadvisor/blob/v0.40.0/container/containerd/handler.go#L196>这里</a>是复用的 libcontainer 的逻辑。那这个 libcontainer 的 handler 具体又是怎么实现 <code>GetStats</code> 方法的呢？</p>
<pre tabindex=0><code>// Get cgroup and networking stats of the specified container
func (h *Handler) GetStats() (*info.ContainerStats, error) {
    ...
    // 主要的监控数据，如 CPU、内存等是读取的 cgroup 数据
    cgroupStats, err := h.cgroupManager.GetStats()
    ...
    libcontainerStats := &amp;libcontainer.Stats{
		CgroupStats: cgroupStats,
	}
    ...

	// If we know the pid then get network stats from /proc/&lt;pid&gt;/net/dev
	if h.pid &gt; 0 {
		if h.includedMetrics.Has(container.NetworkUsageMetrics) {
            // 网络层面的相关监控数据是通过读取 proc 文件系统得到的
            netStats, err := networkStatsFromProc(h.rootFs, h.pid)
        }
        ...
    }
    ...
}</code></pre>
<p>soga，看来主要是依赖 cgroup 还有读取 /proc 文件系统来汇总的监控数据。所以从 docker 切换到 containerd，cadvisor 在监控采集这块的具体实现上差别不大。</p>
<p>当然，前面提到的笔者自己做的一些修改，这次切换到 containerd 以后自然还是要做一下兼容的。做法也挺简单，就是按照社区开发人员建议的，合并这些 flag，然后统一做注入操作，这块笔者也给社区反馈了一个单独的 PR，见 <a href=https://github.com/google/cadvisor/pull/2921>PR #2921</a>。</p>
<h3 id=日志适配-log-pilot>日志适配：log-pilot</h3>
<p>搞定了监控这块以后，笔者又遇到了一个更棘手的问题：日志方案也不好使了。</p>
<p>之前搞日志采集方案的时候，笔者选用的是阿里云容器团队开源的 <a href=https://github.com/AliyunContainerService/log-pilot>log-pilot</a> 项目。它的实现原理说来也挺简单的，就是嵌了一个日志采集器，然后同时监听 docker 的 event，读取相应容器的 volume 信息，再根据指定的 env 或者 label 标签，在自己的目录下生成对应的日志采集器（比如 filebeat 或是 fluentd ）的配置。</p>
<p>log-pilot 原本是监听的 docker event 来做事的，切换到 containerd 以后直接就给整罢工了。</p>
<p>那么，这个方案到底能不能用在 containerd 上面呢？核心即是要解决下面两个问题：</p>
<blockquote>
<ol>
<li><p>containerd 或者 CRI 能否也提供一套 event 机制供订阅消费？</p></li>
<li><p>能否通过 containerd 或者 CRI 接口拿到容器的详细信息（最核心的如容器的 ID、logPath、env、volume mount 等）？</p></li>
</ol>
</blockquote>
<p>这些问题确实还蛮棘手的，笔者也是花了大概一周左右的时间详细了解了一下 containerd 和 cri-api 的相关细节，终于有所收获。</p>
<p>话不多说，直接上代码。</p>
<p>首先，为了解决第一个问题，我们先来看看 log-pilot 是如何消费 docker event 的：</p>
<pre tabindex=0><code>...
msgs, errs := p.client.Events(ctx, options)

go func() {
    ...
    for {
        select {
        case msg := &lt;-msgs:
            if err := p.processEvent(msg); err != nil {
                log.Errorf(&#34;fail to process event: %v,  %v&#34;, msg, err)
            }
        ...
    }
}()</code></pre>
<p>可以看到，log-pilot 确实是调用 docker client 的 <code>Events</code> 方法获取事件信息，然后在拿到消息后执行对应的处理逻辑。那切换到 containerd 之后，怎么获取容器启停的这个事件源呢？翻遍<a href=https://github.com/kubernetes/cri-api/blob/master/pkg/apis/services.go#L98> CRI 接口</a>，笔者并没有找到任何有关 event 的接口：</p>
<pre tabindex=0><code>// RuntimeService interface should be implemented by a container runtime.
// The methods should be thread-safe.
type RuntimeService interface {
	RuntimeVersioner
	ContainerManager
	PodSandboxManager
	ContainerStatsManager

	// UpdateRuntimeConfig updates runtime configuration if specified
	UpdateRuntimeConfig(runtimeConfig *runtimeapi.RuntimeConfig) error
	// Status returns the status of the runtime.
	Status() (*runtimeapi.RuntimeStatus, error)
}</code></pre>
<p>那 containerd 本身有没有提供这方面的接口呢？谷歌搜索可以找到 containerd 仓库里这部分的代码 <a href=https://github.com/containerd/containerd/blob/v1.5.5/events/events.go>events.go</a>：</p>
<pre tabindex=0><code>// Subscriber allows callers to subscribe to events
type Subscriber interface {
	Subscribe(ctx context.Context, filters ...string) (ch &lt;-chan *Envelope, errs &lt;-chan error)
}</code></pre>
<p>containerd 还真有提供事件订阅机制，那么问题来了，怎么用呢？很遗憾，这方面的文档非常稀缺，笔者不得不通过代码里的 <a href=https://github.com/containerd/containerd/blob/v1.5.5/events/exchange/exchange_test.go>testcase</a> 来了解具体的调用方式：</p>
<pre tabindex=0><code>func TestExchangeBasic(t *testing.T) {
	ctx := namespaces.WithNamespace(context.Background(), t.Name())
	testevents := []events.Event{
		&amp;eventstypes.ContainerCreate{ID: &#34;asdf&#34;},
		&amp;eventstypes.ContainerCreate{ID: &#34;qwer&#34;},
		&amp;eventstypes.ContainerCreate{ID: &#34;zxcv&#34;},
	}
	exchange := NewExchange()

	t.Log(&#34;subscribe&#34;)
	var cancel1, cancel2 func()

	// Create two subscribers for same set of events and make sure they
	// traverse the exchange.
	ctx1, cancel1 := context.WithCancel(ctx)
	eventq1, errq1 := exchange.Subscribe(ctx1)
    ...
}</code></pre>
<p>OK，event 这块有了，那么第二个问题怎么解决呢？如何获取容器的明细信息呢？</p>
<p>log-pilot 之前是通过调用 docker client 的 <code>ContainerList</code> 和 <code>ContainerInspect</code> 来获取容器的明细。那么，如今是否可以通过调用 containerd 对外暴露的 CRI 接口来实现这一步呢？如果可以的话，那这块代码便可以自然而然地推广到其他支持 CRI 接口的运行时了。</p>
<p>然而，很遗憾，答案是否定的。这一块也是笔者最困惑的地方，CRI 标准里的确是有列出所有容器和获取某个容器详细状态的<a href=https://github.com/kubernetes/cri-api/blob/v0.22.1/pkg/apis/services.go#L42>接口</a>：</p>
<pre tabindex=0><code>...
// ListContainers lists all containers by filters.
ListContainers(filter *runtimeapi.ContainerFilter) ([]*runtimeapi.Container, error)
// ContainerStatus returns the status of the container.
ContainerStatus(containerID string) (*runtimeapi.ContainerStatus, error)
...</code></pre>
<p>然而，这上面接口返回的信息少了一些关键字段，比如 <a href=https://github.com/kubernetes/cri-api/blob/v0.22.1/pkg/apis/runtime/v1alpha2/api.pb.go#L4141>runtimeapi.Container</a> 里面<strong>根本就没有容器 env 和 volume 相关字段</strong>，更让人迷惑的是，<a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/vendor/k8s.io/cri-api/pkg/apis/services.go#L35>CreateContainer</a> 创建容器接口方法里却要求用户传入容器配置 <a href=https://github.com/kubernetes/cri-api/blob/v0.22.1/pkg/apis/runtime/v1alpha2/api.pb.go#L3390>ContainerConfig</a>，里面就有 env 和 mount 相关的定义。</p>
<p>再者，<a href=https://github.com/kubernetes/cri-api/blob/v0.22.1/pkg/apis/runtime/v1alpha2/api.pb.go#L4366>runtimeapi.ContainerStatus</a> 里面尽管有 mount 字段，却不是全部的 volume mount 数据，比如在 Dockerfile 里通过 <code>VOLUME</code> 指令创建的 image volume 就不包含在内。</p>
<p>事情似乎又走到了死胡同。</p>
<p>诶？等一下，平时敲 <code>crictl inspect &lt;CONTAINER_ID></code> 的时候是可以看到 env 和 volume 的，<code>crictl</code> 这个工具它是怎么拿到的呢？翻阅了 crictl 的<a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/cmd/crictl/container.go#L390>代码实现</a>以后一下就恍然大悟了：</p>
<pre tabindex=0><code>var containerStatusCommand = &amp;cli.Command{
	Name:      &#34;inspect&#34;,
	Usage:     &#34;Display the status of one or more containers&#34;,
	ArgsUsage: &#34;CONTAINER-ID [CONTAINER-ID...]&#34;,
	Action: func(context *cli.Context) error {
        ...
        runtimeClient, runtimeConn, err := getRuntimeClient(context)
        if err != nil {
            return err
        }
        defer closeConnection(context, runtimeConn)

        for i := 0; i &lt; context.NArg(); i++ {
            containerID := context.Args().Get(i)
            err := ContainerStatus(runtimeClient, containerID, context.String(&#34;output&#34;), context.String(&#34;template&#34;), context.Bool(&#34;quiet&#34;))
            if err != nil {
                return errors.Wrapf(err, &#34;getting the status of the container %q&#34;, containerID)
            }
        }
        return nil
    },
}</code></pre>
<p>可以看到，它实例化了一个 <code>runtimeClient</code>，然后获取 container 状态的时候用到了这个 client。那么不妨再来看看这个 client 是啥：</p>
<pre tabindex=0><code>func getRuntimeClient(context *cli.Context) (pb.RuntimeServiceClient, *grpc.ClientConn, error) {
	// Set up a connection to the server.
	conn, err := getRuntimeClientConnection(context)
	if err != nil {
		return nil, nil, errors.Wrap(err, &#34;connect&#34;)
	}
	runtimeClient := pb.NewRuntimeServiceClient(conn)
	return runtimeClient, conn, nil
}</code></pre>
<p>明白了，它在<a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/cmd/crictl/util.go#L182>这里</a>初始化了一个通过 grpc 连接到 containerd 的 client。这个和笔者之前了解到的 CRI 调用方式还有一些出入，笔者以为会是下面这样：</p>
<pre tabindex=0><code>import (
    ...
    internalapi &#34;k8s.io/cri-api/pkg/apis&#34;
    &#34;k8s.io/kubernetes/pkg/kubelet/cri/remote&#34;
    ...
)
// 之前笔者以为 cri-tool 会通过类似这样的方式去初始化一个 CRI 标准的 client 服务
// internalapi.RuntimeService 即是所有对外暴露的 CRI 接口
func getRuntimeService(context *cli.Context) (internalapi.RuntimeService, error) {
	return remote.NewRemoteRuntimeService(RuntimeEndpoint, Timeout)
}</code></pre>
<p>那么，cri-tool 为什么会选择用 grpc 的方式去调用 contaienrd 的接口，而不是标准的 CRI 呢？</p>
<p>对比之下，笔者发现，<code>internalapi.RuntimeService</code> 返回的信息就是标准的 CRI 接口约定的内容，而 grpc 版本的返回里会附带一些额外信息，比如在执行 <code>crictl inspect &lt;CONTAINER_ID></code> 的时候，它在<a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/cmd/crictl/container.go#L866>这里</a>会将获取的一些额外信息也打印输出到屏幕：</p>
<pre tabindex=0><code>return outputStatusInfo(status, r.Info, output, tmplStr)</code></pre>
<p>这里的 <a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/vendor/k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go#L4550>Info</a> 也即是调用 <a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/vendor/k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go#L7781>ContainerStatus</a> 这个 grpc 接口拿到的 <a href=https://github.com/kubernetes-sigs/cri-tools/blob/v1.22.0/vendor/k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go#L4543>ContainerStatusResponse</a> 里的其中一部分。</p>
<p>也就是说，要想拿到容器的 env 和完整的 volume mount 等这些&rdquo;额外信息&rdquo;，光靠 CRI 接口是不够用的（这感觉挺扯淡的）。</p>
<p>话说回来，要是想解决第二个问题，看上去只需要自己手动解析一下这个 <code>Info</code>，然后拿到对应的 env 和 volume 数据就行了。至此，基本都有解决办法了，可以开搞了！</p>
<p>通过少许的改造，笔者最终也是完成了这块的适配工作，详细改动见 <a href=https://github.com/Colstuwjx/log-pilot/pull/2>Colstuwjx/log-pilot PR #2</a>。</p>
<p>在开发过程中，笔者也遇到了一些问题：</p>
<ol>
<li><p>不同于 docker 容器默认输出的 jsonlog，containerd 容器标准输出的日志格式是 plain text，log-pilot 生成的 filebeat、fluentd 配置需要做一些相应的适配；</p></li>
<li><p>containerd 置备的 volume 只是单纯作为卷挂载到容器里，并没有像 docker 那样作为一种资源显式管理起来，比如 docker 可以执行 <code>docker volume ls</code> 来查看对应的卷。因此，我们需要统一封装一层，不能再完全照搬之前 inspect volume 这块的逻辑；</p></li>
<li><p>在切换到 containerd 以后，如果机器上同时还跑着 docker daemon ，再用 docker 去启动一个容器的话，containerd 会监听到一些 exit 类型的垃圾事件，怀疑可能是错误拿到了 docker 容器的相关事件。解决办法也很简单：就是一个宿主机上只跑一种 runtime；</p></li>
<li><p>在实际运行改版后的 log-pilot 后，笔者发现时不时会收到一条 task start 事件，一路追查下来发现，该容器的 ID 竟然是某个 Pod sandbox ID，想必 containerd 是监听到了 Pod 的 sandbox 容器启动事件。这块其实也比较迷惑，按道理 containerd 应该不会再向用户展示 sandbox 容器这一层了，事实上，从命令也可以看到，sandbox 是通过一个单独的命令来查询的，然而在 event 这一层它们又变成对等的 container 了，挺奇怪的逻辑。最终，笔者通过在拿到事件后手动检查一下是否是 Pod Sandbox ID 的 workaround 方式变相解决了这个问题。</p></li>
</ol>
<h2 id=结语>结语</h2>
<p>这一番折腾下来总算搞定了，万万没想到最花时间精力的部分竟然是周边的监控和日志组件的适配。</p>
<p>其实这个过程中间，相信大家也能看出来，k8s 和 docker 两个开发阵营之间的 gap 还是很多的，这里列举几个比较典型的吧：</p>
<ul>
<li><p>cadvisor 在开发的时候压根就没考虑过调用 docker 接口之类的，直接读 cgroup 和 /proc，这个做法也一直延续至今；</p></li>
<li><p>CRI 约定的只是 kubelet 管理容器相关需要的一些接口，至于 events、volume 这些都是没有包含在内的。其实个人认为至少应该把 image volume（即在 Dockerfile 里面通过 <code>VOLUME</code> 指定的匿名卷，它的定义会体现在 OCI 镜像里 ）和 container volume 这些给明确定义出来，而不是放任不管，然后在实现的时候一股脑塞到原生 grpc 接口的 extra info 里&mldr;</p></li>
<li><p>如果想用 crictl 或者类似的工具去调用 containerd 裸起一个容器，相信我，这真不是一件简单的事儿。其实也能理解，这并不是 k8s 社区关注的重点；</p></li>
<li><p>containerd 原生暴露的 event 接口里能够监听到 sandbox 容器 start 的事件，然而在查询容器这层面的接口又无法查到 Sandbox 容器的信息，那么 Sandbox 到底是不是一个 container 呢？似乎和 dockershim 时代的 infra container 相比，Sandbox 这个概念在 containerd 具体实现时变得更加模糊了；</p></li>
<li><p>containerd 自身是有区分 namespace 的，比如 docker 容器默认都在 <code>moby</code> 这个 namespace 下面，k8s 的则会是 <code>k8s.io</code>。此 namespace 非 k8s 的那个彼 namespace。这个 namespace 概念感觉有点类似于 jenkins 的 workspace 的意思，然而它是没有体现到 CRI 标准里的，这也就造成了我们实际在用 containerd 的时候，只有当找不到容器了，才会意识到有这个 namespace 的区分；</p></li>
<li><p>早期的云原生开发者多是站在 docker 用户的角度思考问题，这也是为什么 log-pilot 会直接选择把宿主机目录以只读形式挂载到它的容器里，这样来采集日志。然而，时代变了，今天回头再看 log-pilot 的这个做法，恐怕已经是有悖于云原生理念了，至少在安全性方面是很难达标的；</p></li>
<li><p>docker 的 event 机制乍一看好像和 k8s 的 informer 机制差别不大，但是仔细一想其实出入挺大的：event 传递的是变化的增量信息，informer 除了传递这个变化以外，还鼓励和提倡遵循 &ldquo;reconcile&rdquo; 这套理念。</p></li>
</ul>
<p><em>注：当然，以上这些也只是笔者个人看到的一些情况，未必是完全准确的，欢迎拍砖。</em></p>
<p>此外，这次迁移也给了笔者一次反思的机会。时移世易，有些在之前看起来理所当然的行为模式，如今回过头来看可能会有一些新的看法。</p>
<p>完。</p>
<h2 id=参考>参考</h2>
<ul>
<li><p><a href=https://www.infoq.cn/article/odslclsjvo8bnx*mbrbk>https://www.infoq.cn/article/odslclsjvo8bnx*mbrbk</a></p></li>
<li><p><a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md>https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md</a></p></li>
<li><p><a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/cmd/crictl/container.go>https://github.com/kubernetes-sigs/cri-tools/blob/master/cmd/crictl/container.go</a></p></li>
</ul>
</div>
</div>
<div id=post-footer class="post-footer main-content-wrap">
<div class=post-footer-tags>
<span class="text-color-light text-small">标签</span><br>
<a class="tag tag--primary tag--small" href=https://colstuwjx.github.io/tags/cloud-native/>cloud-native</a>
<a class="tag tag--primary tag--small" href=https://colstuwjx.github.io/tags/containerd/>containerd</a>
<a class="tag tag--primary tag--small" href=https://colstuwjx.github.io/tags/docker/>docker</a>
</div>
<div class=post-actions-wrap>
<nav>
<ul class="post-actions post-action-nav">
<li class=post-action>
<a class="post-action-btn btn btn--disabled">
<i class="fa fa-angle-left"></i>
<span class="hide-xs hide-sm text-small icon-ml">下一篇</span>
</a>
</li>
<li class=post-action>
<a class="post-action-btn btn btn--default tooltip--top" href=https://colstuwjx.github.io/2021/08/%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%BB%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%B1%82%E9%9D%A2%E6%80%9D%E8%80%83-kubernetes-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%BC%83%E7%94%A8%E5%AF%B9-docker-%E7%9A%84%E6%94%AF%E6%8C%81/ data-tooltip="【源码解读】从代码实现层面思考 Kubernetes 为什么会弃用对 Docker 的支持？">
<span class="hide-xs hide-sm text-small icon-mr">上一篇</span>
<i class="fa fa-angle-right"></i>
</a>
</li>
</ul>
</nav>
<ul class="post-actions post-action-share">
<li class="post-action hide-lg hide-md hide-sm">
<a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions>
<i class="fa fa-share-alt"></i>
</a>
</li>
<li class=post-action>
<a class="post-action-btn btn btn--default" href=#>
<i class="fa fa-list"></i>
</a>
</li>
</ul>
</div>
</div>
</article>
<footer id=footer class=main-content-wrap>
<span class=copyrights>
&copy; 2021 Powered by Hugo with tranquilpeak. All Rights Reserved
</span>
</footer>
</div>
<div id=bottom-bar class=post-bottom-bar data-behavior=4>
<div class=post-actions-wrap>
<nav>
<ul class="post-actions post-action-nav">
<li class=post-action>
<a class="post-action-btn btn btn--disabled">
<i class="fa fa-angle-left"></i>
<span class="hide-xs hide-sm text-small icon-ml">下一篇</span>
</a>
</li>
<li class=post-action>
<a class="post-action-btn btn btn--default tooltip--top" href=https://colstuwjx.github.io/2021/08/%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%BB%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%B1%82%E9%9D%A2%E6%80%9D%E8%80%83-kubernetes-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%BC%83%E7%94%A8%E5%AF%B9-docker-%E7%9A%84%E6%94%AF%E6%8C%81/ data-tooltip="【源码解读】从代码实现层面思考 Kubernetes 为什么会弃用对 Docker 的支持？">
<span class="hide-xs hide-sm text-small icon-mr">上一篇</span>
<i class="fa fa-angle-right"></i>
</a>
</li>
</ul>
</nav>
<ul class="post-actions post-action-share">
<li class="post-action hide-lg hide-md hide-sm">
<a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions>
<i class="fa fa-share-alt"></i>
</a>
</li>
<li class=post-action>
<a class="post-action-btn btn btn--default" href=#>
<i class="fa fa-list"></i>
</a>
</li>
</ul>
</div>
</div>
<div id=share-options-bar class=share-options-bar data-behavior=4>
<i id=btn-close-shareoptions class="fa fa-close"></i>
<ul class=share-options>
</ul>
</div>
<div id=share-options-mask class=share-options-mask></div>
</div>
<div id=about>
<div id=about-card>
<div id=about-btn-close>
<i class="fa fa-remove"></i>
</div>
<img id=about-card-picture src=https://res.cloudinary.com/ddhitj0ja/image/upload/v1581899652/me_jfkg8t.jpg alt=作者的图片>
<h4 id=about-card-name>Jacky Wu (Colstuwjx)</h4>
<div id=about-card-bio>DevOPS & OpenSource Fans</div>
<div id=about-card-job>
<i class="fa fa-briefcase"></i>
<br>
DevOPS Engineer
</div>
<div id=about-card-location>
<i class="fa fa-map-marker"></i>
<br>
Shanghai, China
</div>
</div>
</div>
<div id=cover style=background-image:url(https://res.cloudinary.com/ddhitj0ja/image/upload/v1581843208/pic_nfgplt.jpg)></div>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script>
<script src=https://colstuwjx.github.io/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js></script>
<script lang=javascript>window.onload=updateMinWidth,window.onresize=updateMinWidth,document.getElementById("sidebar").addEventListener("transitionend",updateMinWidth);function updateMinWidth(){var b=document.getElementById("sidebar"),a=document.getElementById("main"),c,d,e;a.style.minWidth="",c=getComputedStyle(a).getPropertyValue("min-width"),d=getComputedStyle(b).getPropertyValue("width"),e=getComputedStyle(b).getPropertyValue("left"),a.style.minWidth=`calc(${c} - ${d} - ${e})`}</script>
<script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:!1}),$('pre.code-highlight > code, pre > code').each(function(b,a){$(this).hasClass('codeblock')||$(this).addClass('codeblock'),hljs.highlightBlock(a)})})</script>
</body>
</html>